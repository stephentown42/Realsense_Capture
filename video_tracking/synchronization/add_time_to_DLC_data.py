""" 

Time here is defined relative to the TDT clock, so that we can cross-reference the time of sensor and valve events


Inputs
    - Landmark positions for every frame, generated by DeepLabCut for high and low exposure videos
    - Image alignment (warp) matrices
    - Ongoing frame rates, estimated via the synchronization pipeline

Output
    - Landmarks in a common spatial reference frame with estimated times of each frame

TO DO:
    Add field to video files indicating which type of tracking is available

"""

from datetime import datetime
import logging
from pathlib import Path
import os, sys

from dotenv import load_dotenv
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

sys.path.insert(0, str(Path.cwd()))

from lib import utils
from Methods.video_tracking import loading as vload


def add_final_trial(ongoing_fps: pd.DataFrame) -> pd.DataFrame:
    """ Sessions always end on an incomplete trial, for which we need to add information in order to sync with tracking fully  """

    # Calculate session fps as average weighted by trial duration
    mean_fps = sum(ongoing_fps['fps'] * ongoing_fps['duration']) / sum(ongoing_fps['duration'])

    # Create an extra trial to use for frames after the last synchronization marker
    session_end = pd.DataFrame([{
        'starttimecorrected' : np.inf, 
        'start_frame' : np.inf, 
        'prev_start_frame': ongoing_fps['start_frame'].max(),
        'prev_start_time': ongoing_fps['starttimecorrected'].max(),
        'duration': np.inf,
        'fps': mean_fps}])

    session_end.index += ongoing_fps.shape[0]

    # Combine
    return pd.concat((ongoing_fps, session_end))


def load_ongoing_FPS(file_path:Path, fnum:int, block:str):
    """ Load ongoing FPS estimates (or skip this session if required data not found) """
    
    fps_filepath = file_path / f"F{fnum}_Block_{block}.csv"   
    
    if fps_filepath.exists(): 

        # Load FPS estimates in windows between known events (beginning of file, then every trial onset for which fps estimates were made)
        df = pd.read_csv(
            filepath_or_buffer = fps_filepath,
            usecols=['starttimecorrected','start_frame','prev_start_frame','prev_start_time','fps','duration']
            )
        
        # Remove nans in sync data (not sure why these are present)
        nrows_src = df.shape[0]
        df = df.dropna()
        nrows_rm = nrows_src - df.shape[0]

        if nrows_rm > 0:
            print(f"{fps_filepath.name} contains {nrows_rm} / {nrows_src} rows with nans")

        # Add extra time window from last known state until end of session (i.e. infinity)
        df = add_final_trial(df)

        return df
    else:
        return None


def in_MCS_trials(fnum:int, block:str):
    """ Check to see if there's any data about block in MCS trials (a table that contains corrected start-times)  """

    query = """ 
        SELECT COUNT(*)
        FROM task_switch.mcs_trials_20230219
        WHERE session_dt = (
            SELECT datetime
            FROM task_switch.sessions
            WHERE ferret = %(fnum)s
                AND block = %(block)s);
    """

    df = utils.query_postgres(query, params={'fnum':int(fnum), 'block':block})
    return df['count'].values[0] > 0


def list_videos_with_tdt_timestamps(start_dt:str, end_dt:str) -> pd.DataFrame:
    """ 
    Get metadata on videos containing tdt timestamps (i.e. that use a fundamentally different way of referencing frame time to that based on ongoing frame rate)

    Args:
        start_dt: datetime from which videos started being recorded alongside tdt timestamps
        end_dt: datetime at which videos stopped being recorded alongside tdt timestamps (end of project)
    
    Returns:
        dataframe containing info on video files (rows)

    Notes:
        See [tdt_timestamp_sync.ipynb](./notebooks/tdt_timestamp_sync.ipynb for a broader) for a broader discussion of video files containing tdt timestamps.
    """
    
    query = """ 
        SELECT 
            ferret as fnum,
            block,
            REPLACE( REPLACE(REPLACE(filename, '_resized',''), '.avi', '.dat'), 'Track', 'FrameTDTsamps') as dat_file
        FROM task_switch.video_files
        WHERE session_dt > %(start_dt)s
            AND session_dt < %(end_dt)s
            AND frame_count > 0;
    """

    return utils.query_postgres(query, params={'start_dt':start_dt, 'end_dt':end_dt})


def list_sound_only_videos():

    query = """ 
    WITH CTE AS (
    SELECT 
        ss.ferret,
        ss.block,
        ss.datetime as session_dt,
        SUM( CASE WHEN ii.modality = 'Auditory' THEN 1 ELSE 0 END) as n_auditory,
        COUNT(*) as n_trials
    FROM task_switch.sessions ss
    INNER JOIN task_switch.trials tt
        ON ss.datetime = tt.session_dt
    INNER JOIN task_switch.stimuli ii
        ON tt.stim_id = ii.id
    GROUP BY ss.datetime)

    SELECT 
        vf.ferret as fnum, vf.block, session_dt, n_auditory, n_trials, default_fps,	vf.filename
    FROM task_switch.video_files vf
    INNER JOIN (
        SELECT * 
        FROM CTE 
        WHERE n_auditory = n_trials 
            AND block IS NOT NULL) aud_sessions
        USING(session_dt)
    WHERE rv2 is true
    ORDER BY vf.ferret, vf.block;"""

    return utils.query_postgres(query)


def process_using_ongoing_FPS(dlc_df:pd.DataFrame, fps_df:pd.DataFrame) -> pd.DataFrame:

    # Add timing variables
    dlc_df['true_frame'] = dlc_df.index     # the position of each frame in the video file
    dlc_df['local_frame'] = dlc_df.index    # the position of each frame relative to the most recent synchronization event
    dlc_df['time'] = np.nan     # Time (in seconds) of frame 

    # For every synchronization event
    for _, fps_window in fps_df.iterrows():

        # Get indices of frames after event and before next event 
        frame_limits = (fps_window['prev_start_frame'], fps_window['start_frame'])
        idx = dlc_df.query(f"{frame_limits[0]} < true_frame < {frame_limits[1]}").index

        # Convert frame number to value relative to synchronization event
        dlc_df.loc[idx, 'local_frame'] -= fps_window['prev_start_frame']
        
        # Convert frame to time* using estimated fps (* within sync window)
        dlc_df.loc[idx, 'time'] = (dlc_df.loc[idx, 'local_frame'] / fps_window['fps'])

        # Add the time at which the sync window began, so that time now references full session
        dlc_df.loc[idx, 'time'] += fps_window['prev_start_time']

    # Post_processing to remove helper variables and ensure type consistency
    return (
        dlc_df
        .drop(columns='local_frame')
        .rename(columns={'true_frame':'frame'})
        .astype({'frame': np.uint32})
    )



def main():

    # Settings
    tdt_samplerate = 48848.125
    fallback_fps = 29.866522783879137                   # Calculated from all ongoing fps estimates (see Methods/video_tracking/notebooks/video_metadata.ipynb for more info)
    dlc_tracking_file = 'DLC_aligned_230218_1258.parquet'
    
    # Load environmental variables and define paths
    load_dotenv()

    data_path = Path(os.getenv("local_home")) / 'Task_Switching/head_tracking'
    fps_path = data_path / 'ongoing_fps'
    edge_detect_path = data_path / 'edge_detect_test'
    timestamp_path = Path.cwd()  / 'data/text/tdt_timestamps_4_videos'

    # Get names of blocks for which TDT timestamps are available
    tdt_videos = list_videos_with_tdt_timestamps('2018-01-29 00:00:00.00', '2018-02-28 23:00:00.00')
    tdt_videos = tdt_videos.drop_duplicates()

    # Separate summary files exist for high and low exposure images. 
    input_path = data_path / dlc_tracking_file
    blocks = vload.get_unique_rows_from_parquet(input_path, ['fnum','block'])

    # Get schema from input file and append timing variables
    schema = pq.read_schema( input_path)
    schema = schema.append(pa.field("frame", pa.uint32()))
    schema = schema.append(pa.field("time", pa.float64()))

    # Progress log
    log_filename = 'DLC_alignsync'
    logger = utils.make_logger(data_path, log_filename, add_datetime=True)

    # Identify tdt timestamp files that are on the local machine and log those missing (we expect some - see ./../notebooks/tdt_timestamp_sync.ipynb for more info)
    tdt_videos['on_disk'] = tdt_videos['dat_file'].apply(lambda x: (timestamp_path / x).exists())
    
    for i, tdt_vid in tdt_videos.query("on_disk == False").iterrows():
        logger.warning(f"TDT timestamps not found: F{tdt_vid['fnum']} Block_{tdt_vid['block']}, {tdt_vid['dat_file']}")

    # Check tdt timestamp files for which tracking exists
    tdt_videos = pd.merge(
        left = tdt_videos.query("on_disk == True"),  # Only take forward files for which have tdt samples availabel
        right = blocks.assign(track = True),         # Include track as extra variable so we can highlight any blocks for which tdt samples exist but tracking doesn't (should not happen)
        left_on = ['fnum','block'],
        right_on = ['fnum','block'],
        how = 'left'
    )

    assert tdt_videos.track.all()       # All files should have tracking data, otherwise check DLC file

    # Those blocks for which tdt samples do not exist must be synced using ongoing frame rates
    ofps_blocks = pd.concat((tdt_videos[['fnum','block']], blocks)).drop_duplicates(keep=False)

    # Ongoing frame rate estimates are not available for blocks in which only sounds were presented. Here, we identify those blocks for separate processing and remove them from the list of blocks synced using ongoing frame rates
    auditory_blocks = list_sound_only_videos()
    ofps_blocks = pd.concat((auditory_blocks[['fnum','block']], ofps_blocks)).drop_duplicates(keep=False)

    # Check if ongoing fps estimates exist on local machine
    ofps_blocks['target_file'] = 'F'+ ofps_blocks['fnum'].astype(str) + '_Block_' + ofps_blocks['block'] + '.csv'
    ofps_blocks['on_disk'] = ofps_blocks['target_file'].apply(lambda x: (fps_path / x).exists())

    # Log those blocks without ongoing fps estimates and then keep only those for which estimates are available
    for i, block in ofps_blocks.query("on_disk == False").iterrows():
        logger.warning(f"Ongoing fps estimates not found: F{block['fnum']} Block_{block['block']}")

    ofps_blocks = ofps_blocks.query("on_disk == True")

    # Open parquet writer for saving multiple results
    save_path = data_path / datetime.now().strftime('DLC_alignsync_%y%m%d_%H%M.parquet') 
   
    with pq.ParquetWriter(save_path, schema, compression='gzip') as writer: 

        # Convert frame times using tdt measures
        for (fnum, block), block_df in tdt_videos.groupby(['fnum','block']):  

            logger.info(f"Running F{fnum} Block_{block}")
            
            dlc_df = vload.load_parquet(input_path, fnum, block)
            dlc_df['frame'] = dlc_df.index

            tdt_samps = np.loadtxt( timestamp_path / block_df['dat_file'].values[0], delimiter=',')
            assert len(tdt_samps) == dlc_df.shape[0]           
            dlc_df['time'] = tdt_samps / tdt_samplerate

            writer.write_table(
                    pa.Table.from_pandas(
                        dlc_df.astype({'frame': np.uint32}),
                        preserve_index=False)
                )

        # Estimate frame times using ongoing fps estimates
        for (fnum, block), block_df in ofps_blocks.groupby(['fnum','block']):  

            logger.info(f"Running F{fnum} Block_{block}")
            dlc_df = vload.load_parquet(input_path, fnum, block)

            fps_df = load_ongoing_FPS(fps_path, fnum, block)
            dlc_df = process_using_ongoing_FPS(dlc_df, fps_df)

            writer.write_table(
                pa.Table.from_pandas(dlc_df, preserve_index=False)
            )

        # Estimate frame times using fixed fps value (when no other route is possible, does not include )
        for (fnum, block), block_df in auditory_blocks.groupby(['fnum','block']):

            logger.info(f"Using fallback frame rate ({fallback_fps:.3f} fps) for F{fnum} Block_{block}")
            dlc_df = vload.load_parquet(input_path, fnum, block)

            dlc_df['frame'] = dlc_df.index
            dlc_df['time'] = dlc_df.index / fallback_fps

if __name__ == '__main__':
    main()