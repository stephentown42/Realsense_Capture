""" 
Loading module for head tracking data

Allows import of data from SQL database, parquet files and results files 
generated by DeepLabCut

Version History:
    2022-10-01: Created by Stephen Town
"""

from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple
import re, sys
import warnings

import h5py
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from lib import utils

###################################
# Database queries

def get_trial_times(fnum:int, block:str) -> pd.DataFrame:
    """ Get the times of trial onsets in the video associated with one block
    from SQL database """

    query = """ 
    SELECT  
        ts.starttime,
        vt.starttime_video,
        vt.starttime_video + ts.reaction_time as response_time,
        mcs as starttime_mcs
    FROM 
        (SELECT * from task_switch.sessions WHERE ferret = %(fnum)s AND block = %(block)s) s
    INNER JOIN task_switch.video_timestamps vt
        ON s.datetime = vt.session_dt
    INNER JOIN task_switch.trials ts
        ON s.datetime = ts.session_dt AND vt.starttime = ts.starttime
    INNER JOIN task_switch.mcs_trials_20220314 mcs
        ON s.datetime = mcs.session_dt AND ts.starttime = mcs.starttime
    ORDER BY vt.starttime;
    """

    return utils.query_postgres(query, params={'fnum':fnum, 'block':block})


def list_recording_files_for_block(ferret:int, block:str) -> pd.DataFrame:
    """ 
    Get the files containg neural data that were recorded during a given
    session, where session is defined by the input args.

    Data is sought from postgres database
    
    Blocks may be associated with 0, 1 or 2 recording files
    """

    query = """ 
    select 
        rf.region,
        rf.headstage,
        rf.h5_file
    FROM task_switch.sessions s
    INNER JOIN task_switch.recording_files rf
        ON s.datetime = rf.session_dt
    WHERE
        s.ferret = %(ferret)s
        AND s.block = %(block)s;
     """

    df = utils.query_postgres(query, {"ferret": ferret, "block":block})

    # Warn user if there are no files associated with the session
    if df.shape[0] > 0:
        return df
    else:
        warnings.warn('No recording files found')
        return None


def get_info_for_deeplabcut_file(file_name:str) -> dict:
    """ DeepLabCut returns files containing dates rather than blocks, and so we need a way of recovering
    this information 
    
    >x> get_info_for_deeplabcut_file('2018-02-23_Track_09-57-47DLC_resnet50_FrontiersLED_HEmatSep1shuffle1_400000.csv')
    {
        "session_dt" : 2018-02-23 09:57:46.837
        "ferret" : 1605,
        "block" : "J5-39"
    }
    
    """

    # Look for files beginning with date string kept by DLC
    stub = file_name[:25]+"%"

    query = """ 
        SELECT * 
        FROM task_switch.video_files 
        WHERE filename LIKE %(stub)s;
    """

    return utils.query_postgres(query, params={"stub":stub}).to_dict()


def find_calibration_image(dt:str) -> str:
    """ Finds the most recent calibration image taken in Jumbo before an input date and returning either file name, path or contents

    >>> find_calibration_image('2017-01-01 00:00:00')
    '2016-10-26 10_49_09.jpg'
    """

    query = """
        SELECT 
            image.id as file_name
        FROM task_switch.calibration_images image
        WHERE
            image.datetime < %(dt)s
        ORDER BY image.datetime DESC
        LIMIT 1;       
    """

    df = utils.query_postgres(query, params={'dt':dt})
    df = df.to_dict('records')[0]
    
    return df['file_name']


def get_bounding_boxes(calib_im:str) -> pd.DataFrame():
    """ Get bounding boxes for areas around one or more spouts in calibration image 
    
    >>> get_bounding_boxes('2016-09-27 17_51_31.jpg')
        spout  start_col  start_row  width  height
    0      2        509        130     35      30
    1     10        144        137     39      31
    2     12        330         32     27      35
    """

    query = """
        SELECT spout, start_col, start_row, width, height
        FROM task_switch.bounding_boxes bb
        WHERE bb.calib_im = %(calib_im)s;
    """

    return utils.query_postgres(query, params={'calib_im':calib_im})

##################
# Reading from specific formats
def read_deeplabcut_csv(file_path:str, file_name:Optional[str]=None, nframes:Optional[int]=None):
    """ Load tracking data generated by DeepLabCut """

    # Extend file path
    if file_name:
        file_path = Path(file_path) / file_name

    #  Get column names from second and third rows of dataframe
    cn = pd.read_csv(file_path, skiprows=1, nrows=1).iloc[0].to_dict()
    cn = {k:v for (k, v) in cn.items() if v != 'coords'}

    # Read in data (note that column names are split over two rows, which we need to correct)
    df = pd.read_csv(file_path, skiprows=2, nrows=nframes, index_col='coords')

    # Make column names useful
    new_names = {dfc: re.sub(r'\..','', k+v) for dfc, (k, v) in zip(df.columns, cn.items())}
    df.rename(columns=new_names, inplace=True)
    
    return df


def load_parquet(file_path:str, fnum:Optional[int]=None, block:Optional[str]=None,
    time_range:Optional=None, landmarks:Optional=None, verbose:Optional[bool]=False) -> pd.DataFrame:
    """ Load tracking data generated after compression into Parquet 

    Args:
        file_path: path to .parquet file
        fnum: optional id for subject
        block: optional id for session
        time_range: optional tuple containing start and end times, can only be used if a specific block is given
        landmarks: list of landmark names (e.g. ["red_LED","blue_LED","head"]) to load
        verbose: optional flag for reporting size of input data
    
    Examples:
        >>> load_parquet(./dir/test.parquet)
        >>> load_parquet(./dir/test.parquet, fnum=1506)
        >>> load_parquet(./dir/test.parquet, fnum=1607, block='J2-8')
        >>> load_parquet(./dir/test.parquet, fnum=1607, block='J2-8', time_range=(100, 300))
        >>> load_parquet(./dir/test.parquet, fnum=1607, block='J2-8', time_range=(100, 300), landmarks=['red_LED','blue_LED'])

    See also: 
        - data/arrow/compress_DLC_data.py
        - data/arrow/compress_LED_data.py
    """

    # Input checking
    if block is None:
        if time_range is not None:
            raise ValueError("Time range cannot be used without a specific block")

    if isinstance(landmarks, str):
        landmarks = [landmarks]

    # Build arguments to pass to parquet read_table
    kwargs = dict()   

    # Format filters for subject, session and time
    if fnum is not None:
        kwargs['filters'] = [("fnum", "=", fnum)]

    if block is not None:
        kwargs['filters'].append(("block", "in", [block]))

    if time_range is not None:
        kwargs['filters'].append(("time", ">", time_range[0]))    # after start time
        kwargs['filters'].append(("time", "<", time_range[1]))    # before end time

    # Format columns if selecting specific landmarks
    if landmarks is not None:

        kwargs['columns'] = ['fnum','block']              # default columns (should be in every file)

        schema = pq.read_schema(file_path)       # include time if in file (may not always be the case)
        if "time" in schema.names:
            kwargs['columns'].append('time')

        for lm in landmarks:                    # x,y and likelihood for each landmark
            kwargs['columns'] += [lm+'x', lm+'y', lm+'likelihood']            

    # Load data
    table = pq.read_table(file_path, **kwargs)

    if table.shape[0] == 0:
        return None
            
    # Data integrity checks
    n_ferrets = len(table["fnum"].unique())
    n_blocks = len(table["block"].unique())

    if fnum is not None and n_ferrets != 1:
        raise ValueError(f'Ferrets ({n_ferrets}) is not unique and non-zero')

    if block is not None: 
        if n_blocks == 0:
            raise ValueError('No blocks found ')
        elif n_blocks > 1:
            raise ValueError("Multiple blocks returned")

    if verbose:
        print(f"Loaded {table.shape[0]} rows")

    return table.to_pandas()


def get_unique_rows_from_parquet(file_path, columns):
    """ Get the unique rows from a set of columns in a parquet file """

    table = pq.read_table(file_path, columns=columns)
    return table.to_pandas().drop_duplicates()


def load_spike_times_for_files(spike_file:str, h5_files:pd.DataFrame) -> dict():
    """ 
     Get data set containing multiple channels            

    Args:
        spike_file: .h5 file containing times of action potentials
        h5_files: dataframe contining names of recording files for one or more sessions

    Returns
        spike_times: dict with keys for each electrode in each ession
    """
    
    spike_times = dict()

    with h5py.File(spike_file, "r") as hf:

        # For each recording file        
        for _, rec_file in h5_files.iterrows():

            ds = hf[rec_file['h5_file'].replace('.h5','')]

            # For each recorded channel within session
            for chan in ds.keys():
                key = chan.replace('chan', rec_file['region'])     # Add region to channel label, e.g. from Chan01 to APC01
                spike_times[key] = ds[chan][:] / 10**6              # Data are stored as integer microseconds on MCS clock

    return spike_times



def load_sensor_data(h5_path, ferret, block) -> dict:
    """ Load data after extraction, event detection and saving with compression

    Args:
        h5_path: path to h5 file containing compressed sensor data for file 
        ferret: full name of ferret (e.g. F1613_Ariel)                       <---- TO DO: make this just fnum
        block: full string of block (e.g. Block_J2_23)

    Returns:
        dict containing fields for onsets and offsets, where number of onsets and offsets should be the same
    
    Notes:
        See data/hdf5/compress_sensor_events.py for more info on the source data generation

    >x> sensor_ev = load_sensor_data('file/path/sensors.h5', "F1613_Ariel", "Block_J4-13")
    
    """

    onsets, offsets, on_chan, off_chan = [], [], [], []

    with h5py.File(h5_path, "r") as hf:

        # For each sensor in arenea
        for chan in range(0, 4):

            # Load from file
            onsets_temp = hf[f"{ferret}/{block}/S{chan}_ON"][:]
            offsets_temp = hf[f"{ferret}/{block}/S{chan}_OFF"][:]

            # Deal with edge cases
            if  min(offsets_temp) < min(onsets_temp):   # if begin high, add initial zero for first onset
                onsets_temp = np.hstack((0.0, onsets_temp))

            if max(onsets_temp) > max(offsets_temp):    # if end_high, add inf as last offset (we won't know until later how long the recording was)
                offsets_temp = np.hstack((offsets_temp, np.inf))

            assert len(onsets_temp) == len(offsets_temp)

            # Initialize for first channel
            if chan == 0:
                onsets = onsets_temp
                offsets = offsets_temp
                ev_chan = np.full_like(onsets_temp, chan)
            
            # Stack on every further iteration
            else:
                onsets = np.hstack((onsets, onsets_temp))
                offsets = np.hstack((offsets, offsets_temp))
                ev_chan = np.hstack((ev_chan, np.full_like(onsets_temp, chan)))

    # Convert from microsecond integers into floating point seconds
    onsets = onsets.astype(float) / 1e6
    offsets = offsets.astype(float) / 1e6

    # Return as dictionary
    return {
        'onsets':onsets, 
        'offsets':offsets,
        'chan': ev_chan 
        }    



def main():

    import doctest
    doctest.testmod()

if __name__ == '__main__':
    main()